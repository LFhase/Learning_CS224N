\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{float}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

% No page numbers
%\pagenumbering{gobble}

% MARGINS (DO NOT EDIT) ---------------------------------------------
\oddsidemargin  0in \evensidemargin 0in \topmargin -0.5in
\headheight 0.25in \headsep 0.25in
\textwidth   6.5in \textheight 9in
\parskip 1.5ex  \parindent 0ex \footskip 20pt
% ---------------------------------------------------------------------------------

% HEADER (DO NOT EDIT) -----------------------------------------------
\newcommand{\problemnumber}{0}
\newcommand{\myname}{name}
\newfont{\myfont}{cmssbx10 scaled 1200}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\myfont Question \problemnumber, Assignment 5, CS224n}
%\fancyhead[R]{\bssnine \myname}
\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
% ---------------------------------------------------------------------------------

% BEGIN HOMEWORK HERE
\begin{document}

% Question 1
\newquestion{1}
\begin{enumerate}[label=(\alph*)]
    \item Typically, the vocabulary size of characters is lower than that of words, so a lower dimension of a lookup table is enough for character-embeddings.
    \item $Size_{char}  = e_{char}*V_{char}$ and $Size_{word} = e_{word}*V_{word}$, the second has more parameters by a $\frac{50,000}{96}$.
    \item Since CNN is not sensible to the word positions, it's more robust to noises in the sentence, while RNN relies a lot on the positional information.
    \item Average-pooling makes more use of the information than max-pooling as it aggregates the global information by averaging, while max-pooling only focuses on max-informative part.
\end{enumerate}

\newquestion{2}
Finally the trained NMT System has 395.37 ppl over dev set. The BLEU over testset is 24.43.

\newquestion{3}
\begin{enumerate}[label=(\alph*)]
    \item \textit{traducir} and \textit{traduce} occur. For word-based NMT, though these words have almost the same meaning,
    the occurance count may not count in some of them and <unk> problem will happen because of less appearance of some forms.
    However, char-aware NMT model can repair the issue because their forms are very similar.
    \item 
    \begin{enumerate}[label=(\roman*)]
        \item Word based:
        \begin{enumerate}
            \item financial: economic, business, markets, banking, finance
            \item neuron: nerve, neural, cells, brain, nervous
            \item Francisco: san, jose, diego, antonio, california
            \item naturally: occurring, readily, humans, arise, easily
            \item expectation: norms, assumptions, policies, inflation, confidence
        \end{enumerate}
        \item Char based:
        \begin{enumerate}
            \item financial: vertical, informal, physical, cultural, electrical
            \item neuron: neurons, neurons., neuro, neuron,
            \item Francisco: Francisco, Francisco,, Francisco.
            \item naturally: pratically, typically, significantly, mentally, gradually
            \item expectation: expectations, expectation
        \end{enumerate}
        \item Word based embeddings focuse on the meaning of the word while char based embeddings focuse form.
        So in the results, the closest words for word based embeddings are more closer in terms of meanings than forms.
        By constrast, the closest words for char based embeddings are more closer in terms of forms instead of meanings. 
    \end{enumerate}
    \item 
    \begin{enumerate}[label=(\roman*)]
        \item Incorrect Example \\
        Spanish: Puedo vestirme como agricultor, o con ropa de cuero, y nunca nadie ha elegido un agricultor.\\
        Gold: You can have me as a farmer, or in leathers,  and no one has ever chose farmer. \\
        Char-based: I can dress as a farmer, or with leather clothes, and never nobody has chosen a farmer.\\
        Word-based: I can \textless unk\textgreater like \textless unk\textgreater or with clothing and never chosen a farmer.\\
        Possible Explanation: The multi forms of the word \textit{vestirme} make the model difficult to 
        learn the pontential meaning 'get dressed' and further predict it.\\
        \item Correct Example \\
        Spanish: Bien, al da siguiente estbamos en Cleveland.\\
        Gold: Well, the next day we were in Cleveland.\\
        Char-based: Well, the next day we were in Cleveland.\\
        Word-based: Well, the next day we were in <unk>.\\
        Possible Explanation: The low frequency of \textit{Cleveland} in the vocabulary makes the model difficult to 
        learn effective information about the word and further predict it.
    \end{enumerate}
\end{enumerate}



\end{document}
